[
  {
    "title": "Implement SQL transformation logic for flatten, split, combine operations",
    "body": "## Description\nThe SQL Database Project generation currently has TODO placeholders for transformation logic.\n\n## Location\n`SqlProject/SqlDatabaseProjectService.cs` lines 530-555\n\n## Tasks\n- [ ] Implement flatten nested objects transformation\n- [ ] Implement array splitting logic\n- [ ] Implement field combination transformation\n- [ ] Implement type conversion logic\n- [ ] Implement custom transformation extensibility\n- [ ] Add unit tests for each transformation type\n- [ ] Update documentation with transformation examples\n\n## Acceptance Criteria\n- Generated SQL scripts include working transformation logic\n- Transformations handle common Cosmos DB scenarios (e.g., nested addresses, arrays of items)\n- Unit tests cover all transformation types with realistic data\n- Documentation includes examples of each transformation type\n\n## Related Issues\n- Related to #2 (child table detection)\n- Blocks #7 (post-migration validation)\n\n## Code References\n```csharp\n// Current placeholder code:\nswitch (transformationRule.TransformationType.ToLower())\n{\n    case \"flatten\":\n        sb.AppendLine(\"        -- TODO: Implement specific flattening logic based on source schema\");\n        break;\n    // ... more TODOs\n}\n```",
    "labels": ["priority: critical", "type: technical-debt", "area: sql-project", "size: L"],
    "milestone": "v1.2.0 - Core Improvements"
  },
  {
    "title": "Detect NestedObject vs Array for child table schema generation",
    "body": "## Description\nCurrently hardcoded to assume all child tables are arrays. Need to inspect JSON structure to determine actual type.\n\n## Location\n`Services/CosmosDbAnalysisService.cs` line 564\n\n## Current Code\n```csharp\nChildTableType = \"Array\", // TODO: Detect if it's NestedObject vs Array\n```\n\n## Implementation\n- Inspect JSON structure to determine if nested field is object or array\n- Set `ChildTableType` accordingly (\"NestedObject\" or \"Array\")\n- Update schema recommendation logic based on type\n- Handle mixed scenarios (sometimes object, sometimes array)\n\n## Acceptance Criteria\n- Correctly identifies nested objects vs arrays\n- Different table structures generated for each type\n- Tests validate both scenarios with sample Cosmos DB documents\n- Documentation explains the detection logic\n\n## Related Issues\n- Related to #1 (transformation logic)\n- Impacts SQL schema generation quality\n\n## Examples\n```json\n// Array scenario\n{\"tags\": [\"tag1\", \"tag2\"]}\n\n// Nested object scenario  \n{\"address\": {\"street\": \"123 Main\", \"city\": \"Seattle\"}}\n```",
    "labels": ["priority: critical", "type: bug", "area: cosmos-analysis", "size: M"],
    "milestone": "v1.2.0 - Core Improvements"
  },
  {
    "title": "Add EstimatedRowCount to ContainerMapping model",
    "body": "## Description\nLarge table warnings cannot be generated due to missing `EstimatedRowCount` property in the model.\n\n## Location\n`Services/SqlProjectIntegrationService.cs` line 321\n\n## Current Code\n```csharp\n.Where(cm => true) // Placeholder - we don't have EstimatedRowCount in the model\n```\n\n## Tasks\n- [ ] Add `EstimatedRowCount` property to `ContainerMapping` model\n- [ ] Query Cosmos DB for document counts during analysis\n- [ ] Populate property in `CosmosDbAnalysisService`\n- [ ] Update `SqlProjectIntegrationService` to use actual counts\n- [ ] Add migration size warnings (>10M rows, >100GB)\n- [ ] Update reports to show estimated row counts\n\n## Acceptance Criteria\n- Model includes `EstimatedRowCount` property\n- Accurate counts retrieved from Cosmos DB\n- Warnings generated for large containers\n- Excel reports display row count estimates\n\n## Related Issues\n- Related to #8 (incremental migration planning)\n- Impacts migration time estimates\n\n## Warning Thresholds\n- ðŸŸ¡ Warning: >1M rows\n- ðŸŸ  High priority: >10M rows  \n- ðŸ”´ Critical: >100M rows or >500GB",
    "labels": ["priority: high", "type: enhancement", "area: cosmos-analysis", "size: M"],
    "milestone": "v1.2.0 - Core Improvements"
  },
  {
    "title": "Add cancellation token support to report generation",
    "body": "## Description\nLong-running Excel/Word generation cannot be cancelled gracefully.\n\n## Location\n`Reporting/ReportGenerationService.cs`\n\n## Tasks\n- [ ] Add `CancellationToken` parameter to all report generation methods\n- [ ] Check cancellation between major operations (worksheet creation, data writing)\n- [ ] Update tests to verify cancellation behavior\n- [ ] Update `Program.cs` to pass cancellation token from main method\n- [ ] Clean up partial files if cancelled\n\n## Acceptance Criteria\n- All public report methods accept `CancellationToken`\n- Cancellation is checked at appropriate intervals\n- Partial files are cleaned up on cancellation\n- Tests verify cancellation works correctly\n- User sees clear \"Operation cancelled\" message\n\n## Implementation Notes\n```csharp\n// Add to method signatures\npublic async Task GenerateExcelReportAsync(\n    AssessmentResult assessment,\n    string outputPath,\n    CancellationToken cancellationToken = default)\n\n// Check during operations\ncancellationToken.ThrowIfCancellationRequested();\n```",
    "labels": ["priority: high", "type: enhancement", "area: reporting", "size: M"],
    "milestone": "v1.2.0 - Core Improvements"
  },
  {
    "title": "Implement --test-connection command",
    "body": "## Description\nDocumentation references `--test-connection` flag that doesn't exist in code. Need to implement connectivity testing.\n\n## Location\n- `Program.cs` (add new command)\n- Referenced in `docs/getting-started.md` line 139\n\n## Tasks\n- [ ] Add `--test-connection` to `CommandLineOptions`\n- [ ] Implement `TestConnectionAsync` method\n- [ ] Test Cosmos DB connectivity (list databases)\n- [ ] Test Azure Monitor connectivity (if configured)\n- [ ] Test Azure authentication status\n- [ ] Display detailed connection status with checkmarks\n- [ ] Update help text\n\n## Acceptance Criteria\n- Command tests all configured connections\n- Provides clear success/failure messages with troubleshooting hints\n- Exits gracefully without running full assessment\n- Works with all authentication methods (CLI, Managed Identity, Service Principal)\n\n## Example Output\n```\nðŸ” Testing Azure Connections...\n\nâœ… Azure Authentication: Successful (Azure CLI)\nâœ… Cosmos DB Connection: Successful\n   â€¢ Endpoint: https://contoso-cosmos.documents.azure.com:443/\n   â€¢ Databases found: 3\nâœ… Azure Monitor: Successful  \n   â€¢ Workspace: 12345678-1234-1234-1234-123456789012\n   â€¢ Metrics available: Yes\n\nâœ… All connections successful!\n```",
    "labels": ["priority: high", "type: enhancement", "area: cli", "size: S"],
    "milestone": "v1.2.0 - Core Improvements"
  },
  {
    "title": "Implement pre-migration data quality checks",
    "body": "## Description\nAdd comprehensive data quality analysis before migration to identify potential issues.\n\n## Features\n- [ ] Null value analysis by field (% nulls, impact on NOT NULL constraints)\n- [ ] Duplicate detection (by ID and business keys)\n- [ ] Data type consistency checks (type mismatches across documents)\n- [ ] Outlier detection for numeric fields (extreme values)\n- [ ] String length analysis for VARCHAR sizing recommendations\n- [ ] Character encoding issues (non-ASCII, special characters)\n- [ ] Invalid date/timestamp values\n- [ ] Generate data quality report section in Excel\n\n## Acceptance Criteria\n- Quality report included in assessment with dedicated worksheet\n- Actionable recommendations for data cleanup\n- Severity ratings (ðŸ”´ Critical / ðŸŸ¡ Warning / â„¹ï¸ Info)\n- Sample problematic records provided for investigation\n- Statistics on data quality issues found\n\n## Example Findings\n```\nðŸ”´ Critical: 15% of documents have NULL customerEmail (required for PK)\nðŸŸ¡ Warning: 234 duplicate customerIds found across containers\nâ„¹ï¸ Info: Max string length for 'description' is 8,432 chars (consider TEXT type)\n```\n\n## Related Issues\n- Works with #7 (post-migration validation)\n- Helps prevent migration failures",
    "labels": ["priority: medium", "type: enhancement", "area: cosmos-analysis", "size: L"],
    "milestone": "v1.3.0 - Enhanced Migration Features"
  },
  {
    "title": "Generate post-migration validation scripts",
    "body": "## Description\nCreate automated SQL scripts to validate migration success and data integrity.\n\n## Tasks\n- [ ] Row count comparison queries (Cosmos vs SQL)\n- [ ] Checksum validation scripts (sample data verification)\n- [ ] Sample data comparison (first/last N rows)\n- [ ] Foreign key integrity checks\n- [ ] Index existence validation\n- [ ] Performance baseline queries\n- [ ] Generate validation report template\n- [ ] PowerShell script to run all validations\n\n## Generated Files\n- `Scripts/PostMigration/01-RowCountValidation.sql`\n- `Scripts/PostMigration/02-DataIntegrityChecks.sql`\n- `Scripts/PostMigration/03-SampleDataComparison.sql`\n- `Scripts/PostMigration/04-PerformanceBaseline.sql`\n- `Scripts/PostMigration/RunAllValidations.ps1`\n\n## Acceptance Criteria\n- Scripts validate all migrated tables\n- Clear pass/fail results for each validation\n- Automated reporting of discrepancies\n- Documentation on how to run validations\n\n## Related Issues\n- Depends on #6 (data quality checks)\n- Works with #10 (ADF pipeline generation)",
    "labels": ["priority: medium", "type: enhancement", "area: sql-project", "size: L"],
    "milestone": "v1.3.0 - Enhanced Migration Features"
  },
  {
    "title": "Add incremental migration support with change feed",
    "body": "## Description\nSupport phased migrations with incremental sync using Cosmos DB change feed.\n\n## Tasks\n- [ ] Detect change feed availability on containers\n- [ ] Estimate initial load vs incremental sync time\n- [ ] Calculate cutover downtime window\n- [ ] Generate phased migration plan\n- [ ] Time-based partitioning strategies\n- [ ] Document sync process in reports\n- [ ] Change feed processor implementation guidance\n\n## Deliverables\n- Migration phases documented in assessment\n- Incremental sync time estimates\n- Cutover checklist and runbook\n- Change feed configuration recommendations\n\n## Migration Phases\n1. **Initial Load**: Historical data migration (bulk copy)\n2. **Incremental Sync**: Change feed-based replication\n3. **Cutover Prep**: Final sync, validation\n4. **Cutover**: Switch applications to SQL\n5. **Verification**: Post-cutover validation\n\n## Acceptance Criteria\n- Detects change feed enabled containers\n- Provides realistic timeline for each phase\n- Includes rollback procedures\n- Documents sync lag monitoring\n\n## Related Issues\n- Requires #3 (row count estimation)\n- Enhances #10 (ADF pipeline generation)",
    "labels": ["priority: medium", "type: enhancement", "area: data-factory", "size: XL"],
    "milestone": "v1.3.0 - Enhanced Migration Features"
  },
  {
    "title": "Implement Azure Monitor auto-discovery",
    "body": "## Description\nFully automate Azure Monitor workspace discovery to reduce manual configuration.\n\n## Tasks\n- [ ] Parse Cosmos account name from endpoint URL\n- [ ] Query Azure Resource Graph for subscription/resource group\n- [ ] Find linked Log Analytics workspace\n- [ ] Cache discovery results to avoid repeated queries\n- [ ] Add `--skip-auto-discovery` flag for manual configuration\n- [ ] Update configuration documentation\n- [ ] Handle multi-subscription scenarios\n\n## Dependencies\n- Requires `Azure.ResourceManager` SDK\n- Requires `Azure.ResourceManager.Monitor` package\n\n## Acceptance Criteria\n- Zero manual configuration needed in happy path\n- Works across subscriptions (with proper permissions)\n- Falls back gracefully if workspace not found\n- Logs discovery process for troubleshooting\n\n## Implementation Notes\n```csharp\n// Parse from endpoint\nvar accountName = ParseAccountName(\"https://contoso-cosmos.documents.azure.com:443/\");\n// accountName = \"contoso-cosmos\"\n\n// Query Resource Graph\nvar resources = await QueryResourceGraphAsync(\n    $\"Resources | where type == 'Microsoft.DocumentDB/databaseAccounts' and name == '{accountName}'\");\n```",
    "labels": ["priority: medium", "type: enhancement", "area: authentication", "size: M"],
    "milestone": "v1.3.0 - Enhanced Migration Features"
  },
  {
    "title": "Generate Azure Data Factory pipeline JSON",
    "body": "## Description\nCreate ready-to-deploy ADF pipelines based on assessment for migration execution.\n\n## Features\n- [ ] Copy activity for each container â†’ table mapping\n- [ ] Parameterized pipelines (environment-agnostic)\n- [ ] Error handling and retry logic\n- [ ] Monitoring and logging configuration\n- [ ] Data validation activities\n- [ ] Generate ARM template for pipeline deployment\n- [ ] Incremental load support (if change feed detected)\n\n## Generated Files\n- `ADF/Pipelines/MainMigrationPipeline.json`\n- `ADF/Datasets/CosmosDBSource.json`\n- `ADF/Datasets/AzureSQLSink.json`\n- `ADF/LinkedServices/Templates.json`\n- `ADF/deploy.ps1` (deployment script)\n- `ADF/README.md` (usage instructions)\n\n## Pipeline Features\n- Dynamic content for flexible configuration\n- Parallel copy activities (configurable degree)\n- Failure notifications (email, webhook)\n- Progress tracking and logging\n- Rollback procedures\n\n## Acceptance Criteria\n- Valid ADF pipeline JSON generated\n- Pipelines deploy successfully via ARM template\n- Documentation covers customization\n- Tested with sample migrations\n\n## Related Issues\n- Integrates with #7 (validation scripts)\n- Enhanced by #8 (incremental migration)",
    "labels": ["priority: medium", "type: enhancement", "area: data-factory", "size: L"],
    "milestone": "v1.3.0 - Enhanced Migration Features"
  },
  {
    "title": "Add interactive wizard mode",
    "body": "## Description\nImplement step-by-step interactive wizard for user-friendly configuration.\n\n## Tasks\n- [ ] Add `--interactive` flag\n- [ ] Step-by-step prompts for configuration\n- [ ] Real-time input validation\n- [ ] Show progress during analysis\n- [ ] Summary confirmation before execution\n- [ ] Save configuration for reuse\n- [ ] Support for resuming interrupted sessions\n\n## User Flow\n```\nðŸš€ Cosmos DB to SQL Migration Assessment Tool\n\nWelcome! This wizard will guide you through the assessment process.\n\nStep 1/5: Azure Authentication\n  â„¹ï¸ Current status: Authenticated via Azure CLI\n  âœ… Continue with current authentication? [Y/n]: \n\nStep 2/5: Cosmos DB Configuration\n  ðŸ“ Enter Cosmos DB endpoint: https://contoso-cosmos.documents.azure.com:443/\n  âœ… Connection successful! Found 3 databases.\n  \n  Select databases to analyze:\n  [x] 1. ProductCatalog (45 GB, 12 containers)\n  [ ] 2. CustomerData (120 GB, 8 containers)\n  [x] 3. OrderHistory (89 GB, 15 containers)\n  \n  ...\n```\n\n## Acceptance Criteria\n- User-friendly prompts with helpful hints\n- Input validation before proceeding\n- Can exit and resume later\n- Generates same output as CLI mode",
    "labels": ["priority: low", "type: enhancement", "area: cli", "size: M"],
    "milestone": "v1.3.0 - Enhanced Migration Features"
  },
  {
    "title": "Create API reference documentation",
    "body": "## Description\nDocument service interfaces and extension points for developers who want to customize or extend the tool.\n\n## Tasks\n- [ ] Create `docs/api-reference.md`\n- [ ] Document all service interfaces\n- [ ] Model schema documentation with examples\n- [ ] Extension point examples (custom transformations, report formats)\n- [ ] Configuration options reference\n- [ ] Code examples for customization\n- [ ] Integration examples (calling from other apps)\n\n## Sections\n1. **Service Architecture**\n   - Dependency injection setup\n   - Service lifetimes\n   - Interface contracts\n\n2. **Core Services**\n   - `ICosmosDbAnalysisService`\n   - `ISqlMigrationAssessmentService`\n   - `IDataFactoryEstimateService`\n   - `IReportGenerationService`\n\n3. **Models**\n   - `CosmosAnalysis`\n   - `SqlAssessment`\n   - `AssessmentResult`\n   - Schema models\n\n4. **Extension Points**\n   - Custom transformations\n   - Custom report formats\n   - Custom metrics collection\n\n5. **Examples**\n   - Programmatic usage\n   - Custom transformation implementation\n   - Report customization\n\n## Acceptance Criteria\n- Complete API coverage\n- Runnable code examples\n- Links to source code\n- Version compatibility notes",
    "labels": ["priority: medium", "type: documentation", "size: M"],
    "milestone": "Documentation & Quality"
  },
  {
    "title": "Replace placeholder security scanning with actual tools",
    "body": "## Description\nCI/CD pipeline has placeholder for security scanning that needs real implementation.\n\n## Location\n`.github/workflows/build.yml` line 60\n\n## Tasks\n- [ ] Add Microsoft Security DevOps GitHub Action\n- [ ] Enable Dependabot for dependency scanning\n- [ ] Configure GitHub secret scanning\n- [ ] Add SAST analysis (SonarCloud or similar)\n- [ ] Set up security alerts and notifications\n- [ ] Add security badge to README\n- [ ] Document security practices\n\n## Tools to Integrate\n1. **Microsoft Security DevOps**: Multi-tool aggregator\n2. **Dependabot**: Dependency vulnerability scanning\n3. **CodeQL**: Already configured, ensure it's active\n4. **Credential Scanning**: Detect committed secrets\n5. **SBOM Generation**: Software Bill of Materials\n\n## Acceptance Criteria\n- Security scans run on every PR\n- Vulnerabilities block merges (configurable threshold)\n- Weekly dependency update PRs from Dependabot\n- Security alerts routed to maintainers\n- Documentation includes security process\n\n## Example Workflow\n```yaml\n- name: Run Microsoft Security DevOps\n  uses: microsoft/security-devops-action@v1\n  with:\n    tools: 'credscan,terrascan,trivy'\n```",
    "labels": ["priority: medium", "type: cicd", "area: cicd", "size: S"],
    "milestone": "Documentation & Quality"
  },
  {
    "title": "Add performance benchmarking suite",
    "body": "## Description\nCreate performance testing to ensure tool scales with large datasets.\n\n## Tasks\n- [ ] Benchmark schema analysis (1K, 10K, 100K, 1M documents)\n- [ ] Memory profiling tests\n- [ ] Report generation performance (Excel/Word)\n- [ ] SQL project generation performance\n- [ ] Identify optimization opportunities\n- [ ] Document performance characteristics\n- [ ] Set performance regression guards in CI\n\n## Benchmark Scenarios\n1. **Small**: 1K docs, 5 containers, simple schema\n2. **Medium**: 10K docs, 20 containers, moderate nesting\n3. **Large**: 100K docs, 50 containers, complex schema\n4. **XLarge**: 1M docs, 100 containers, deep nesting\n\n## Metrics to Track\n- Time to analyze (per scenario)\n- Peak memory usage\n- Report generation time\n- Disk I/O operations\n\n## Acceptance Criteria\n- Automated benchmark suite runs in CI\n- Performance baselines documented\n- Regression alerts if >20% slower\n- Optimization recommendations documented\n\n## Tools\n- BenchmarkDotNet for .NET\n- Memory profilers (dotMemory)\n- Custom performance harness",
    "labels": ["priority: low", "type: testing", "size: M"],
    "milestone": "Documentation & Quality"
  }
]
